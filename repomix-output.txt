This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.vercel/
  project.json
  README.txt
scrapers/
  coindesk_scraper.py
  generate_embeddings.py
  Pipfile
.gitignore
next.config.js
README_EMBEDDINGS.md

================================================================
Files
================================================================

================
File: .vercel/project.json
================
{"orgId":"GJinFs672IPGEXfXntQldoiG","projectId":"prj_zwCwQmYDKE4kEWQdwlAqNbKKuC3t"}

================
File: .vercel/README.txt
================
> Why do I have a folder named ".vercel" in my project?
The ".vercel" folder is created when you link a directory to a Vercel project.

> What does the "project.json" file contain?
The "project.json" file contains:
- The ID of the Vercel project that you linked ("projectId")
- The ID of the user or team your Vercel project is owned by ("orgId")

> Should I commit the ".vercel" folder?
No, you should not share the ".vercel" folder with anyone.
Upon creation, it will be automatically added to your ".gitignore" file.

================
File: scrapers/coindesk_scraper.py
================
import os
import csv
import time
import random
import traceback
from datetime import datetime
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup


class CoindeskScraper:
    def __init__(self, use_proxy=False, delay_min=2, delay_max=5):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.use_proxy = use_proxy
        self.proxy_url = "grifterProxy"
        self.proxy_username = "us"
        self.proxy_password = "pw"
        self.output_csv = f"coindesk_articles_{self.timestamp}.csv"
        self.base_url = "https://www.coindesk.com"
        self.latest_news_url = f"{self.base_url}/latest-crypto-news"
        self.delay_min = delay_min
        self.delay_max = delay_max
        self.processed_urls = set()  # Track processed URLs to avoid duplicates

        # Initialize CSV file with headers
        with open(self.output_csv, 'w', newline='', encoding='utf-8') as csvfile:
            writer = csv.writer(csvfile)
            writer.writerow([
                'Title',
                'URL',
                'Author',
                'Date',
                'Category',
                'Summary',
                'Content',
                'Tags'
            ])

    def scrape_homepage(self):
        """Scrape the homepage to get article links"""
        try:
            with sync_playwright() as pw:
                browser_options = {}
                context_options = {
                    "user_agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    "bypass_csp": True,
                    "ignore_https_errors": True
                }

                # Add proxy settings if enabled
                if self.use_proxy:
                    context_options["proxy"] = {
                        "server": f"http://{self.proxy_url}",
                        "username": self.proxy_username,
                        "password": self.proxy_password
                    }

                browser = pw.chromium.launch(headless=False, **browser_options)
                context = browser.new_context(**context_options)

                # Add extra headers to appear more like a real browser
                context.set_extra_http_headers({
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.5",
                    "Accept-Encoding": "gzip, deflate, br",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Sec-Fetch-User": "?1",
                    "Cache-Control": "max-age=0",
                })

                page = context.new_page()
                print(f"Accessing Coindesk homepage: {self.latest_news_url}")

                # Use a more lenient wait condition and longer timeout
                try:
                    # First try with domcontentloaded which is faster
                    response = page.goto(
                        self.latest_news_url, wait_until='domcontentloaded', timeout=30000)

                    # Wait a bit for dynamic content to load
                    page.wait_for_timeout(5000)

                except Exception as e:
                    print(f"Initial page load failed: {e}")
                    print("Trying alternative approach...")

                    # Try a different approach - go to main site first
                    response = page.goto(
                        self.base_url, wait_until='domcontentloaded', timeout=30000)
                    page.wait_for_timeout(3000)

                    # Then navigate to latest news
                    response = page.goto(
                        self.latest_news_url, wait_until='domcontentloaded', timeout=30000)
                    page.wait_for_timeout(5000)

                if response is None:
                    print("No response received. Attempting to continue anyway...")
                elif response.status != 200:
                    print(f"Warning: Received status code {response.status}")

                # Extract article links
                article_links = []

                # Try to wait for content to load
                try:
                    # Wait for any link to appear
                    page.wait_for_selector('a[href]', timeout=10000)
                except Exception as e:
                    print(f"Warning: Timeout waiting for links: {e}")

                # Take a screenshot for debugging
                page.screenshot(
                    path=f"coindesk_screenshot_{self.timestamp}.png")
                print(
                    f"Saved screenshot to coindesk_screenshot_{self.timestamp}.png")

                # Get all article links - try multiple approaches
                try:
                    # First try to find article cards or containers
                    articles = page.query_selector_all(
                        'article a[href], .article-card a[href], .story-card a[href], .article a[href]')

                    if not articles or len(articles) < 5:
                        print(
                            "Few article-specific links found, trying generic links...")
                        # Fall back to all links if specific article selectors don't work
                        articles = page.query_selector_all('a[href]')

                    print(f"Found {len(articles)} potential article links")

                    for article in articles:
                        href = article.get_attribute('href')
                        if not href:
                            continue

                        # Make relative URLs absolute
                        if href.startswith('/'):
                            href = f"{self.base_url}{href}"
                        elif not href.startswith('http'):
                            continue

                        # Filter for article links (typically they contain year/month/day in URL)
                        # Also accept other article patterns
                        if (('/20' in href or '/article/' in href or '/news/' in href) and
                            'coindesk.com' in href and
                                href not in article_links):
                            article_links.append(href)

                except Exception as e:
                    print(f"Error extracting links: {e}")
                    traceback.print_exc()

                # If we still don't have links, try extracting from HTML directly
                if not article_links:
                    print("Trying to extract links from HTML directly...")
                    html_content = page.content()
                    soup = BeautifulSoup(html_content, 'html.parser')

                    for a_tag in soup.find_all('a', href=True):
                        href = a_tag['href']

                        # Make relative URLs absolute
                        if href.startswith('/'):
                            href = f"{self.base_url}{href}"
                        elif not href.startswith('http'):
                            continue

                        # Filter for article links
                        if (('/20' in href or '/article/' in href or '/news/' in href) and
                                'coindesk.com' in href):
                            article_links.append(href)

                # Remove duplicates while preserving order
                unique_links = []
                seen = set()
                for link in article_links:
                    if link not in seen:
                        seen.add(link)
                        unique_links.append(link)

                print(f"Found {len(unique_links)} unique article links")

                if len(unique_links) == 0:
                    print(
                        "Warning: No article links found. The site structure may have changed.")

                browser.close()
                return unique_links

        except Exception as e:
            print(f"An error occurred while scraping the homepage")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {e}")
            traceback.print_exc()
            return []

    def scrape_article(self, article_url):
        """Scrape a single article page"""
        # Skip if already processed
        if article_url in self.processed_urls:
            print(f"Skipping already processed article: {article_url}")
            return None

        # Add random delay to avoid detection
        delay = random.uniform(self.delay_min, self.delay_max)
        print(f"Waiting {delay:.2f} seconds before accessing next article...")
        time.sleep(delay)

        try:
            with sync_playwright() as pw:
                browser_options = {}
                context_options = {
                    "user_agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    "bypass_csp": True,
                    "ignore_https_errors": True
                }

                # Add proxy settings if enabled
                if self.use_proxy:
                    context_options["proxy"] = {
                        "server": f"http://{self.proxy_url}",
                        "username": self.proxy_username,
                        "password": self.proxy_password
                    }

                browser = pw.chromium.launch(headless=True, **browser_options)
                context = browser.new_context(**context_options)

                # Add extra headers to appear more like a real browser
                context.set_extra_http_headers({
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
                    "Accept-Language": "en-US,en;q=0.5",
                    "Accept-Encoding": "gzip, deflate, br",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Sec-Fetch-User": "?1",
                    "Cache-Control": "max-age=0",
                })

                page = context.new_page()
                print(f"Accessing article: {article_url}")

                # Use a more lenient wait condition
                response = page.goto(
                    article_url, wait_until='domcontentloaded', timeout=30000)

                # Wait a bit for dynamic content to load
                page.wait_for_timeout(3000)

                if response is None:
                    print("No response received. Attempting to continue anyway...")
                elif response.status != 200:
                    print(f"Warning: Received status code {response.status}")
                    if response.status >= 400:
                        browser.close()
                        return None

                # Mark as processed
                self.processed_urls.add(article_url)

                # Extract article data
                article_data = {}

                # Use BeautifulSoup for easier parsing
                soup = BeautifulSoup(page.content(), 'html.parser')

                # Title
                title_element = soup.select_one('h1')
                article_data['title'] = title_element.text.strip(
                ) if title_element else "No title found"

                # URL
                article_data['url'] = article_url

                # Author
                author_element = soup.select_one('a[href*="/author/"]')
                if not author_element:
                    # Try alternative selectors
                    author_element = soup.select_one(
                        '.at-author__name') or soup.select_one('[data-test-id="byline"]')
                article_data['author'] = author_element.text.strip(
                ) if author_element else "Unknown author"

                # Date
                date_element = soup.select_one('time')
                if date_element and date_element.get('datetime'):
                    article_data['date'] = date_element.get('datetime')
                else:
                    # Try alternative date selectors
                    date_text = None
                    date_candidates = [
                        soup.select_one('.at-created'),
                        soup.select_one('.article-date'),
                        soup.select_one('[data-test-id="published-date"]')
                    ]
                    for candidate in date_candidates:
                        if candidate:
                            date_text = candidate.text.strip()
                            break
                    article_data['date'] = date_text if date_text else "Unknown date"

                # Category
                category_element = soup.select_one('a[href*="/tag/"]') or soup.select_one(
                    'a[href*="/markets/"]') or soup.select_one('a[href*="/business/"]')
                article_data['category'] = category_element.text.strip(
                ) if category_element else "Uncategorized"

                # Summary
                summary_element = soup.select_one('h2') or soup.select_one(
                    '.article-summary') or soup.select_one('.at-subhead')
                article_data['summary'] = summary_element.text.strip(
                ) if summary_element else "No summary available"

                # Content
                # Try multiple selectors for content
                content_elements = soup.select(
                    '.article-body p') or soup.select('.at-content-wrapper p') or soup.select('article p')

                if content_elements:
                    # Filter out empty paragraphs and join with spaces
                    content_text = ' '.join(
                        [p.text.strip() for p in content_elements if p.text.strip()])
                    article_data['content'] = content_text if content_text else "No content available"
                else:
                    article_data['content'] = "No content available"

                # Tags
                tag_elements = soup.select('a[href*="/tag/"]')
                article_data['tags'] = ', '.join(
                    [tag.text.strip() for tag in tag_elements]) if tag_elements else "No tags"

                browser.close()

                # Save to CSV
                self.save_to_csv(article_data)

                return article_data

        except Exception as e:
            print(f"An error occurred while scraping article {article_url}")
            print(f"Error type: {type(e).__name__}")
            print(f"Error message: {e}")
            traceback.print_exc()
            return None

    def save_to_csv(self, article_data):
        """Save article data to CSV file"""
        try:
            with open(self.output_csv, 'a', newline='', encoding='utf-8') as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow([
                    article_data.get('title', ''),
                    article_data.get('url', ''),
                    article_data.get('author', ''),
                    article_data.get('date', ''),
                    article_data.get('category', ''),
                    article_data.get('summary', ''),
                    article_data.get('content', ''),
                    article_data.get('tags', '')
                ])
            print(
                f"Saved article: {article_data.get('title', 'Unknown title')}")
            return True
        except Exception as e:
            print(f"Error saving to CSV: {e}")
            return False


def main():
    # Set use_proxy to False to disable proxy
    # Set delay_min and delay_max to control the random delay between requests
    scraper = CoindeskScraper(use_proxy=False, delay_min=2, delay_max=5)

    # Get article links from homepage
    article_links = scraper.scrape_homepage()

    # Limit to first 20 articles to avoid overloading
    article_links = article_links[:20]

    # Scrape each article
    successful = 0
    for i, link in enumerate(article_links):
        print(f"\nScraping article {i+1} of {len(article_links)}...")
        result = scraper.scrape_article(link)
        if result:
            successful += 1

    print(
        f"\nScraped {successful} out of {len(article_links)} articles successfully")
    print(f"Results saved to {scraper.output_csv}")


if __name__ == "__main__":
    main()

================
File: scrapers/generate_embeddings.py
================
#!/usr/bin/env python3
"""
Generate embeddings from Coindesk articles and store them in databases.

This script:
1. Reads article data from a CSV file
2. Generates embeddings using OpenAI's API
3. Stores the embeddings in both a Neon Postgres database and a local SQLite database
4. Provides utility functions for querying the embeddings

Dependencies:
- pandas
- openai
- sqlalchemy
- psycopg2-binary
- python-dotenv
- tqdm

Install with:
pipenv install pandas openai sqlalchemy psycopg2-binary python-dotenv tqdm
"""

import os
import sys
import csv
import json
import time
import argparse
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple

import pandas as pd
import numpy as np
from tqdm import tqdm
from dotenv import load_dotenv
import openai
from sqlalchemy import create_engine, Column, Integer, String, Float, Text, DateTime, ForeignKey, MetaData, Table
from sqlalchemy.orm import declarative_base, sessionmaker, relationship
from sqlalchemy.dialects.postgresql import JSONB, ARRAY

# Load environment variables from .env file
load_dotenv()

# Configuration
DEFAULT_CSV_PATH = "coindesk_articles_20250315_183049.csv"
DEFAULT_SQLITE_PATH = "coindesk_embeddings.db"
DEFAULT_EMBEDDING_MODEL = "text-embedding-3-small"
DEFAULT_EMBEDDING_DIMENSION = 1536  # Dimension for text-embedding-3-small
DEFAULT_BATCH_SIZE = 10
DEFAULT_CHUNK_SIZE = 8000  # Max tokens for embedding model

# Initialize OpenAI client
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("Error: OPENAI_API_KEY environment variable not set.")
    print("Please set it in a .env file or export it in your shell.")
    sys.exit(1)

client = openai.OpenAI(api_key=openai_api_key)

# Database setup
Base = declarative_base()


class Article(Base):
    """SQLAlchemy model for articles table."""
    __tablename__ = 'articles'

    id = Column(Integer, primary_key=True)
    title = Column(String(500), nullable=False)
    url = Column(String(1000), unique=True)
    author = Column(String(255))
    date = Column(String(255))
    category = Column(String(255))
    summary = Column(Text)
    content = Column(Text)
    tags = Column(String(500))
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationship to embeddings
    embeddings = relationship(
        "Embedding", back_populates="article", cascade="all, delete-orphan")

    def __repr__(self):
        return f"<Article(id={self.id}, title='{self.title[:30]}...', author='{self.author}')>"


class Embedding(Base):
    """SQLAlchemy model for embeddings table."""
    __tablename__ = 'embeddings'

    id = Column(Integer, primary_key=True)
    article_id = Column(Integer, ForeignKey('articles.id', ondelete='CASCADE'))
    model = Column(String(100), nullable=False)
    # 'title', 'content', 'combined'
    embedding_type = Column(String(50), nullable=False)

    # SQLite doesn't support arrays, so we'll store as JSON string
    vector_json = Column(Text)

    # For Postgres, we'll use the ARRAY type - but this is only defined in Postgres tables
    # vector_array is removed from here and added dynamically for Postgres

    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationship to article
    article = relationship("Article", back_populates="embeddings")

    def __repr__(self):
        return f"<Embedding(id={self.id}, article_id={self.article_id}, type='{self.embedding_type}')>"


def setup_database_connections(postgres_url: Optional[str] = None) -> Tuple[Any, Any]:
    """
    Set up connections to both SQLite and Postgres databases.

    Args:
        postgres_url: Connection URL for Postgres database

    Returns:
        Tuple of (sqlite_engine, postgres_engine)
    """
    # SQLite connection
    sqlite_engine = create_engine(f"sqlite:///{DEFAULT_SQLITE_PATH}")

    # Create tables in SQLite
    Base.metadata.create_all(sqlite_engine)

    # Postgres connection (if URL provided)
    postgres_engine = None
    if postgres_url:
        postgres_engine = create_engine(postgres_url)
    else:
        # Try to get from environment
        neon_postgres_url = os.getenv("NEON_POSTGRES_URL")
        if neon_postgres_url:
            postgres_engine = create_engine(neon_postgres_url)
        else:
            print("Warning: No Postgres URL provided. Only using SQLite database.")

    # Create tables in Postgres if available
    if postgres_engine:
        # For Postgres, we need to handle the ARRAY type differently
        # Create tables with SQLAlchemy core instead of ORM
        metadata = MetaData()

        articles = Table(
            'articles', metadata,
            Column('id', Integer, primary_key=True),
            Column('title', String(500), nullable=False),
            Column('url', String(1000), unique=True),
            Column('author', String(255)),
            Column('date', String(255)),
            Column('category', String(255)),
            Column('summary', Text),
            Column('content', Text),
            Column('tags', String(500)),
            Column('created_at', DateTime, default=datetime.utcnow)
        )

        embeddings = Table(
            'embeddings', metadata,
            Column('id', Integer, primary_key=True),
            Column('article_id', Integer, ForeignKey(
                'articles.id', ondelete='CASCADE')),
            Column('model', String(100), nullable=False),
            Column('embedding_type', String(50), nullable=False),
            Column('vector_json', Text),  # Keep this for compatibility
            Column('vector_array', ARRAY(Float)),  # Postgres-specific column
            Column('created_at', DateTime, default=datetime.utcnow)
        )

        metadata.create_all(postgres_engine)

    return sqlite_engine, postgres_engine


def read_articles_from_csv(csv_path: str) -> pd.DataFrame:
    """
    Read articles from CSV file into a pandas DataFrame.

    Args:
        csv_path: Path to the CSV file

    Returns:
        DataFrame containing article data
    """
    try:
        df = pd.read_csv(csv_path)
        print(f"Read {len(df)} articles from {csv_path}")
        return df
    except Exception as e:
        print(f"Error reading CSV file: {e}")
        sys.exit(1)


def generate_embedding(text: str, model: str = DEFAULT_EMBEDDING_MODEL) -> List[float]:
    """
    Generate embedding for a text using OpenAI API.

    Args:
        text: Text to generate embedding for
        model: OpenAI embedding model to use

    Returns:
        List of floats representing the embedding vector
    """
    try:
        # Truncate text if it's too long
        if len(text) > DEFAULT_CHUNK_SIZE:
            text = text[:DEFAULT_CHUNK_SIZE]

        response = client.embeddings.create(
            model=model,
            input=text,
            encoding_format="float"
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error generating embedding: {e}")
        return []


def process_articles(df: pd.DataFrame, model: str = DEFAULT_EMBEDDING_MODEL) -> List[Dict[str, Any]]:
    """
    Process articles and generate embeddings.

    Args:
        df: DataFrame containing article data
        model: OpenAI embedding model to use

    Returns:
        List of dictionaries containing article data and embeddings
    """
    results = []

    for i, row in tqdm(df.iterrows(), total=len(df), desc="Generating embeddings"):
        article_data = row.to_dict()

        # Prepare text for embeddings
        title = article_data.get('Title', '')
        content = article_data.get('Content', '')
        summary = article_data.get('Summary', '')

        # Skip if no meaningful content
        if title == '' and (content == '' or content == 'No content available'):
            print(f"Skipping article {i} due to lack of content")
            continue

        # Generate embeddings for different parts
        title_embedding = generate_embedding(title, model) if title else []

        # Use content if available, otherwise use summary
        if content and content != 'No content available':
            content_embedding = generate_embedding(content, model)
        elif summary and summary != 'No summary available':
            content_embedding = generate_embedding(summary, model)
        else:
            content_embedding = []

        # Generate combined embedding if both title and content are available
        combined_text = f"{title}\n\n{content if content != 'No content available' else summary}"
        combined_embedding = generate_embedding(combined_text, model)

        # Add embeddings to article data
        article_data['title_embedding'] = title_embedding
        article_data['content_embedding'] = content_embedding
        article_data['combined_embedding'] = combined_embedding

        results.append(article_data)

        # Sleep to avoid rate limits
        time.sleep(0.5)

    return results


def store_in_sqlite(articles_with_embeddings: List[Dict[str, Any]], engine: Any) -> None:
    """
    Store articles and embeddings in SQLite database.

    Args:
        articles_with_embeddings: List of dictionaries containing article data and embeddings
        engine: SQLAlchemy engine for SQLite
    """
    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        for article_data in tqdm(articles_with_embeddings, desc="Storing in SQLite"):
            # Create article record
            article = Article(
                title=article_data.get('Title', ''),
                url=article_data.get('URL', ''),
                author=article_data.get('Author', ''),
                date=article_data.get('Date', ''),
                category=article_data.get('Category', ''),
                summary=article_data.get('Summary', ''),
                content=article_data.get('Content', ''),
                tags=article_data.get('Tags', '')
            )
            session.add(article)
            session.flush()  # Get the article ID

            # Create embedding records
            if article_data.get('title_embedding'):
                title_embedding = Embedding(
                    article_id=article.id,
                    model=DEFAULT_EMBEDDING_MODEL,
                    embedding_type='title',
                    vector_json=json.dumps(article_data['title_embedding'])
                )
                session.add(title_embedding)

            if article_data.get('content_embedding'):
                content_embedding = Embedding(
                    article_id=article.id,
                    model=DEFAULT_EMBEDDING_MODEL,
                    embedding_type='content',
                    vector_json=json.dumps(article_data['content_embedding'])
                )
                session.add(content_embedding)

            if article_data.get('combined_embedding'):
                combined_embedding = Embedding(
                    article_id=article.id,
                    model=DEFAULT_EMBEDDING_MODEL,
                    embedding_type='combined',
                    vector_json=json.dumps(article_data['combined_embedding'])
                )
                session.add(combined_embedding)

        session.commit()
        print(
            f"Successfully stored {len(articles_with_embeddings)} articles in SQLite database")
    except Exception as e:
        session.rollback()
        print(f"Error storing in SQLite: {e}")
    finally:
        session.close()


def store_in_postgres(articles_with_embeddings: List[Dict[str, Any]], engine: Any) -> None:
    """
    Store articles and embeddings in Postgres database.

    Args:
        articles_with_embeddings: List of dictionaries containing article data and embeddings
        engine: SQLAlchemy engine for Postgres
    """
    if not engine:
        print("Skipping Postgres storage as no connection is available")
        return

    # Use SQLAlchemy Core for better performance with arrays
    metadata = MetaData()
    articles_table = Table('articles', metadata, autoload_with=engine)
    embeddings_table = Table('embeddings', metadata, autoload_with=engine)

    connection = engine.connect()
    transaction = connection.begin()

    try:
        for article_data in tqdm(articles_with_embeddings, desc="Storing in Postgres"):
            # Insert article
            article_result = connection.execute(
                articles_table.insert().values(
                    title=article_data.get('Title', ''),
                    url=article_data.get('URL', ''),
                    author=article_data.get('Author', ''),
                    date=article_data.get('Date', ''),
                    category=article_data.get('Category', ''),
                    summary=article_data.get('Summary', ''),
                    content=article_data.get('Content', ''),
                    tags=article_data.get('Tags', ''),
                    created_at=datetime.utcnow()
                ).returning(articles_table.c.id)
            )
            article_id = article_result.fetchone()[0]

            # Insert embeddings
            if article_data.get('title_embedding'):
                connection.execute(
                    embeddings_table.insert().values(
                        article_id=article_id,
                        model=DEFAULT_EMBEDDING_MODEL,
                        embedding_type='title',
                        vector_json=json.dumps(
                            article_data['title_embedding']),
                        vector_array=article_data['title_embedding'],
                        created_at=datetime.utcnow()
                    )
                )

            if article_data.get('content_embedding'):
                connection.execute(
                    embeddings_table.insert().values(
                        article_id=article_id,
                        model=DEFAULT_EMBEDDING_MODEL,
                        embedding_type='content',
                        vector_json=json.dumps(
                            article_data['content_embedding']),
                        vector_array=article_data['content_embedding'],
                        created_at=datetime.utcnow()
                    )
                )

            if article_data.get('combined_embedding'):
                connection.execute(
                    embeddings_table.insert().values(
                        article_id=article_id,
                        model=DEFAULT_EMBEDDING_MODEL,
                        embedding_type='combined',
                        vector_json=json.dumps(
                            article_data['combined_embedding']),
                        vector_array=article_data['combined_embedding'],
                        created_at=datetime.utcnow()
                    )
                )

        transaction.commit()
        print(
            f"Successfully stored {len(articles_with_embeddings)} articles in Postgres database")
    except Exception as e:
        transaction.rollback()
        print(f"Error storing in Postgres: {e}")
    finally:
        connection.close()


def similarity_search(query_text: str, engine: Any, embedding_type: str = 'combined', top_k: int = 5) -> List[Dict[str, Any]]:
    """
    Perform similarity search using embeddings.

    Args:
        query_text: Query text to search for
        engine: SQLAlchemy engine (SQLite or Postgres)
        embedding_type: Type of embedding to use ('title', 'content', or 'combined')
        top_k: Number of results to return

    Returns:
        List of dictionaries containing article data and similarity scores
    """
    # Generate embedding for query
    query_embedding = generate_embedding(query_text)

    # Convert to numpy array for easier calculations
    query_embedding_np = np.array(query_embedding)

    # Get all embeddings of the specified type
    Session = sessionmaker(bind=engine)
    session = Session()

    try:
        # Get all articles and their embeddings
        results = session.query(Article, Embedding).join(
            Embedding, Article.id == Embedding.article_id
        ).filter(
            Embedding.embedding_type == embedding_type
        ).all()

        similarities = []

        for article, embedding in results:
            # Get embedding vector
            if engine.url.drivername == 'sqlite':
                # For SQLite, parse the JSON string
                vector = np.array(json.loads(embedding.vector_json))
            else:
                # For Postgres, use the array directly
                vector = np.array(embedding.vector_array)

            # Calculate cosine similarity
            similarity = np.dot(query_embedding_np, vector) / (
                np.linalg.norm(query_embedding_np) * np.linalg.norm(vector)
            )

            similarities.append({
                'article_id': article.id,
                'title': article.title,
                'url': article.url,
                'author': article.author,
                'date': article.date,
                'category': article.category,
                'summary': article.summary,
                'content': article.content,
                'tags': article.tags,
                'similarity': float(similarity)
            })

        # Sort by similarity (descending)
        similarities.sort(key=lambda x: x['similarity'], reverse=True)

        # Return top k results
        return similarities[:top_k]

    finally:
        session.close()


def main():
    """Main function to run the script."""
    parser = argparse.ArgumentParser(
        description='Generate embeddings from Coindesk articles')
    parser.add_argument('--csv', type=str,
                        default=DEFAULT_CSV_PATH, help='Path to CSV file')
    parser.add_argument('--postgres-url', type=str,
                        help='Postgres connection URL')
    parser.add_argument(
        '--model', type=str, default=DEFAULT_EMBEDDING_MODEL, help='OpenAI embedding model')
    parser.add_argument('--search', type=str,
                        help='Perform similarity search with this query')
    args = parser.parse_args()

    # Set up database connections
    sqlite_engine, postgres_engine = setup_database_connections(
        args.postgres_url)

    # If search query provided, perform search and exit
    if args.search:
        print(f"Performing similarity search for: {args.search}")
        results = similarity_search(args.search, sqlite_engine)
        print("\nSearch Results:")
        for i, result in enumerate(results):
            print(
                f"\n{i+1}. {result['title']} (Score: {result['similarity']:.4f})")
            print(f"   Author: {result['author']}")
            print(f"   Category: {result['category']}")
            print(f"   URL: {result['url']}")
        return

    # Read articles from CSV
    csv_path = os.path.join(os.path.dirname(
        os.path.abspath(__file__)), args.csv)
    df = read_articles_from_csv(csv_path)

    # Process articles and generate embeddings
    articles_with_embeddings = process_articles(df, args.model)

    # Store in SQLite
    store_in_sqlite(articles_with_embeddings, sqlite_engine)

    # Store in Postgres (if available)
    if postgres_engine:
        store_in_postgres(articles_with_embeddings, postgres_engine)

    print("Done!")


if __name__ == "__main__":
    main()

================
File: scrapers/Pipfile
================
[[source]]
url = "https://pypi.org/simple"
verify_ssl = true
name = "pypi"

[packages]
sqlalchemy = "*"
pandas = "*"
openai = "*"
psycopg2-binary = "*"
python-dotenv = "*"
tqdm = "*"
playwright = "*"
beautifulsoup4 = "*"

[dev-packages]

[requires]
python_version = "3.11"

================
File: .gitignore
================
node_modules
.env

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
.env
.venv
venv/
ENV/

# Pipenv
.python-version
Pipfile.lock

# Database
*.db
*.sqlite
*.sqlite3

# IDE
.idea/
.vscode/
*.swp
*.swo

# Project specific
*.csv
*.png

================
File: next.config.js
================
/**
 * Run `build` or `dev` with `SKIP_ENV_VALIDATION` to skip env validation. This is especially useful
 * for Docker builds.
 */
import "./src/env.js";

/** @type {import("next").NextConfig} */
const config = {};

export default config;

================
File: README_EMBEDDINGS.md
================
# Coindesk Article Embeddings Generator

This tool generates embeddings from Coindesk articles and stores them in both a local SQLite database and a Neon Postgres database for vector search capabilities.

## Features

- Reads article data from CSV files generated by the Coindesk scraper
- Generates embeddings using OpenAI's API (text-embedding-3-small model)
- Stores embeddings in both SQLite and Postgres databases
- Provides similarity search functionality to find related articles

## Setup

1. Install dependencies:

```bash
# Install dependencies
pipenv install

# Or if you want to install them manually:
pipenv install pandas openai sqlalchemy psycopg2-binary python-dotenv tqdm numpy
```

2. Create a `.env` file:

Copy the `.env.example` file to `.env` and fill in your API keys and database connection strings:

```bash
cp .env.example .env
# Then edit the .env file with your credentials
```

3. Make sure you have a CSV file with Coindesk articles (generated by the scraper)

## Usage

### Generate Embeddings

```bash
# Basic usage (uses default CSV file)
pipenv run python generate_embeddings.py

# Specify a different CSV file
pipenv run python generate_embeddings.py --csv your_articles.csv

# Specify a Postgres URL directly (instead of using .env)
pipenv run python generate_embeddings.py --postgres-url postgresql://user:pass@host:port/db

# Use a different embedding model
pipenv run python generate_embeddings.py --model text-embedding-3-large
```

### Search for Similar Articles

Once you've generated embeddings, you can search for similar articles:

```bash
# Search for articles related to a query
pipenv run python generate_embeddings.py --search "bitcoin price prediction"
```

## Database Schema

The script creates two tables in both databases:

### Articles Table

- `id`: Primary key
- `title`: Article title
- `url`: Article URL (unique)
- `author`: Article author
- `date`: Publication date
- `category`: Article category
- `summary`: Article summary
- `content`: Full article content
- `tags`: Article tags
- `created_at`: Timestamp when the record was created

### Embeddings Table

- `id`: Primary key
- `article_id`: Foreign key to articles table
- `model`: Embedding model used (e.g., "text-embedding-3-small")
- `embedding_type`: Type of embedding ("title", "content", or "combined")
- `vector_json`: Embedding vector stored as JSON (SQLite only)
- `vector_array`: Embedding vector stored as array (Postgres only)
- `created_at`: Timestamp when the record was created

## Notes

- The script generates three types of embeddings for each article:
  - `title`: Embedding of just the article title
  - `content`: Embedding of the article content (or summary if content is not available)
  - `combined`: Embedding of title and content combined
- By default, similarity search uses the "combined" embedding type
- The SQLite database is stored in `coindesk_embeddings.db` in the current directory
- For Postgres, vector similarity search can be optimized with pgvector extension



================================================================
End of Codebase
================================================================
